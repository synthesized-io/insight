{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action='ignore', message='numpy.dtype size changed')\n",
    "warnings.filterwarnings(action='ignore', message='compiletime version 3.5 of module')\n",
    "\n",
    "if not 'workbookDir' in globals():\n",
    "    workbookDir = os.getcwd()\n",
    "os.chdir(os.path.split(workbookDir)[0])\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import ks_2samp\n",
    "import seaborn as sns\n",
    "from synthesized import BasicSynthesizer\n",
    "from synthesized.testing.synthetic_distributions import *\n",
    "from synthesized.testing.synthetic_distributions import _plot_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_values(synthesizer):\n",
    "    print(' ', 'value types:')\n",
    "    for value in synthesizer.values:\n",
    "        print(' ', value.name, value)\n",
    "    print()\n",
    "\n",
    "def log_distance(df_original, df_synthesized):\n",
    "    distances = list()\n",
    "    for name in df_original:\n",
    "        distance = ks_2samp(df_original[name], df_synthesized[name]).statistic\n",
    "        suffix = ''\n",
    "        if distance < 0.05:\n",
    "            suffix += '*'\n",
    "        if distance < 0.01:\n",
    "            suffix += '*'\n",
    "        distances.append('{:.3f}{}'.format(distance, suffix))\n",
    "    print('/'.join(distances), end='  ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 100000\n",
    "\n",
    "def synthesize(df_original, num_iterations, num_logging, fn_logging=log_distance, **kwargs):\n",
    "    start = time.time()\n",
    "    with BasicSynthesizer(df_original, **kwargs) as synthesizer:\n",
    "        log_values(synthesizer)\n",
    "        value_types = {value.name: type(value) for value in synthesizer.values}\n",
    "        df_synthesized = synthesizer.synthesize(num_rows=len(df_original))\n",
    "        fn_logging(df_original, df_synthesized)\n",
    "        for _ in range(num_iterations // num_logging):\n",
    "            synthesizer.learn(data=df_original, num_iterations=num_logging)\n",
    "            df_synthesized = synthesizer.synthesize(num_rows=len(df_original))\n",
    "            assert len(df_synthesized) == len(df_original)\n",
    "            fn_logging(df_original, df_synthesized)\n",
    "        print()\n",
    "        print(' ', 'took', time.time() - start, 's')\n",
    "        return synthesizer.synthesize(num_rows=len(df_original)), value_types\n",
    "\n",
    "def plot(data, synthesized, value_types):\n",
    "#     distances = [ks_2samp(data[col], synthesized[col])[0] for col in data.columns]\n",
    "#     avg_distance = np.mean(distances)\n",
    "#     evaluation[name + '_avg_distance'] = avg_distance\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5), sharex=True, sharey=True)\n",
    "    ax1.set_title('original')\n",
    "    ax2.set_title('synthesized')\n",
    "    _plot_data(data, ax=ax1, value_types=value_types)\n",
    "    _plot_data(synthesized, ax=ax2, value_types=value_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Single-column categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting hyperparameters: entropy regularization, learning rate and decay, beta loss.\n",
    "\n",
    "Only change between different categorical distributions: entropy regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Bernoulli(0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = create_bernoulli(probability=0.3, size=size)\n",
    "df_synthesized, value_types = synthesize(\n",
    "    df_original=df_original, summarizer='summaries', num_iterations=1000, num_logging=20,\n",
    "    # VAE distribution\n",
    "    distribution='normal', latent_size=512,\n",
    "    # Network\n",
    "    network='mlp', capacity=256, depth=4, batchnorm=True, activation='relu',\n",
    "    # Optimizer\n",
    "    optimizer='adam', learning_rate=1e-4, decay_steps=500, decay_rate=0.5,\n",
    "    clip_gradients=1.0, batch_size=128,\n",
    "    # Losses\n",
    "    categorical_weight=1.0, continuous_weight=0.1, beta=5e-3, weight_decay=3e-5,\n",
    "    # Categorical\n",
    "    smoothing=0.0, moving_average=False, similarity_regularization=0.0,\n",
    "    entropy_regularization=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "### (b) Bernoulli(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "df_original = create_bernoulli(probability=0.1, size=size)\n",
    "df_synthesized, value_types = synthesize(\n",
    "    df_original=df_original, summarizer='summaries', num_iterations=1000, num_logging=20,\n",
    "    # VAE distribution\n",
    "    distribution='normal', latent_size=512,\n",
    "    # Network\n",
    "    network='mlp', capacity=256, depth=4, batchnorm=True, activation='relu',\n",
    "    # Optimizer\n",
    "    optimizer='adam', learning_rate=1e-4, decay_steps=500, decay_rate=0.5,\n",
    "    clip_gradients=1.0, batch_size=128,\n",
    "    # Losses\n",
    "    categorical_weight=1.0, continuous_weight=0.1, beta=5e-3, weight_decay=3e-5,\n",
    "    # Categorical\n",
    "    smoothing=0.0, moving_average=False, similarity_regularization=0.0,\n",
    "    entropy_regularization=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "### (c) Categorical([0.5, 0.25, 0.125, ...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  value types:\n",
      "  x categorical5-229\n",
      "\n",
      "0.479  0.051  0.065  0.045*  0.051  0.064  0.043*  0.039*  0.026*  0.031*  0.033*  0.033*  0.026*  0.033*  0.031*  0.028*  0.033*  0.031*  0.027*  0.030*  0.035*  0.030*  0.025*  0.017*  0.009**  0.007**  0.010**  0.014*  0.012*  0.013*  0.014*  0.014*  0.015*  0.016*  0.016*  0.016*  0.016*  0.016*  0.015*  0.013*  0.013*  0.015*  0.014*  0.015*  0.014*  0.013*  0.011*  0.010*  0.010**  0.011*  0.012*  \n",
      "  took 77.79881310462952 s\n"
     ]
    }
   ],
   "source": [
    "df_original = create_categorical(probabilities=[0.5, 0.25, 0.125, 0.0625, 0.0625], size=size)\n",
    "df_synthesized, value_types = synthesize(\n",
    "    df_original=df_original, summarizer='summaries', num_iterations=1000, num_logging=20,\n",
    "    # VAE distribution\n",
    "    distribution='normal', latent_size=512,\n",
    "    # Network\n",
    "    network='mlp', capacity=256, depth=4, batchnorm=True, activation='relu',\n",
    "    # Optimizer\n",
    "    optimizer='adam', learning_rate=3e-4, decay_steps=300, decay_rate=0.5,\n",
    "    initial_boost=True, clip_gradients=1.0, batch_size=128,\n",
    "    # Losses\n",
    "    categorical_weight=40.0, continuous_weight=12.0, beta=5e-3, weight_decay=0.0,\n",
    "    # Categorical\n",
    "    temperature=0.2, smoothing=0.0, moving_average=False,\n",
    "    similarity_regularization=0.0, entropy_regularization=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Single-column continuous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting hyperparameters: learning rate and decay, beta loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "### (a) Gaussian(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  value types:\n",
      "  x continuous-gamma\n",
      "\n",
      "0.249  0.147  0.176  0.151  0.156  0.156  0.159  0.134  0.140  0.140  0.129  0.126  0.147  0.115  0.133  0.119  0.106  0.104  0.112  0.098  "
     ]
    }
   ],
   "source": [
    "df_original = create_1d_gaussian(mean=0.0, std=1.0, size=size)\n",
    "df_synthesized, value_types = synthesize(\n",
    "    df_original=df_original, summarizer='summaries', num_iterations=1000, num_logging=20,\n",
    "    # VAE distribution\n",
    "    distribution='normal', latent_size=512,\n",
    "    # Network\n",
    "    network='mlp', capacity=256, depth=4, batchnorm=True, activation='relu',\n",
    "    # Optimizer\n",
    "    optimizer='adam', learning_rate=1e-4, decay_steps=500, decay_rate=0.5,\n",
    "    clip_gradients=1.0, batch_size=128,\n",
    "    # Losses\n",
    "    categorical_weight=1.0, continuous_weight=20.0, beta=1.0, weight_decay=0.0,\n",
    "    # Categorical\n",
    "    smoothing=0.0, moving_average=False, similarity_regularization=0.0,\n",
    "    entropy_regularization=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Gaussian mixture N(0, 1) and N(3, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that fitting of distributions is not stable and for the same parameters of original distribution we can get a different fitted distribution. We actually should not fit any distribution in this case. Shouldn't we decrease threshold for distribution fitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = create_two_gaussian_mixtures(mean1=0.0, std1=1.0, mean2=3.0, std2=1.0, size=size)\n",
    "df_synthesized, value_types = synthesize(\n",
    "    df_original=df_original, summarizer='summaries', num_iterations=1000, num_logging=20,\n",
    "    # VAE distribution\n",
    "    distribution='normal', latent_size=512,\n",
    "    # Network\n",
    "    network='mlp', capacity=256, depth=4, batchnorm=True, activation='relu',\n",
    "    # Optimizer\n",
    "    optimizer='adam', learning_rate=1e-3, decay_steps=500, decay_rate=0.5,\n",
    "    clip_gradients=1.0, batch_size=128,\n",
    "    # Losses\n",
    "    categorical_weight=1.0, continuous_weight=0.1, beta=5e-3, weight_decay=3e-5,\n",
    "    # Categorical\n",
    "    smoothing=0.0, moving_average=False, similarity_regularization=0.0,\n",
    "    entropy_regularization=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (i) Production-like settings which sould produce reasonable result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = create_two_gaussian_mixtures(mean1=0.0, std1=1.0, mean2=3.0, std2=1.0, size=size)\n",
    "df_synthesized, value_types = synthesize(\n",
    "    df_original=df_original, summarizer=None, num_iterations=5000, num_logging=100,\n",
    "    # VAE distribution\n",
    "    distribution='normal', latent_size=512,\n",
    "    # Network\n",
    "    network='mlp', capacity=128, depth=2, batchnorm=True, activation='relu',\n",
    "    # Optimizer\n",
    "    optimizer='adam', learning_rate=3e-4, decay_steps=1000, decay_rate=0.9,\n",
    "    clip_gradients=1.0, batch_size=64,\n",
    "    # Losses\n",
    "    categorical_weight=1.0, continuous_weight=0.1, beta=0.001, weight_decay=1e-5,\n",
    "    # Categorical\n",
    "    smoothing=0.0, moving_average=True, similarity_regularization=0.0,\n",
    "    entropy_regularization=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original.hist(bins=100)\n",
    "df_synthesized.hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Two-column product distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Bernoulli(0.1) x Bernoulli(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same hyperparameters as for single-column case, but slightly increased entropy regularization value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = product(\n",
    "    df1=create_bernoulli(probability=0.1, size=size),\n",
    "    df2=create_bernoulli(probability=0.1, size=size)\n",
    ")\n",
    "df_synthesized, value_types = synthesize(\n",
    "    df_original=df_original, summarizer=None, num_iterations=1000, num_logging=20,\n",
    "    # VAE distribution\n",
    "    distribution='normal', latent_size=512,\n",
    "    # Network\n",
    "    network='mlp', capacity=256, depth=2, batchnorm=True, activation='relu',\n",
    "    # Optimizer\n",
    "    optimizer='adam', learning_rate=1e-5,\n",
    "    decay_steps=200,  # !!!\n",
    "    decay_rate=0.5, clip_gradients=1.0, batch_size=128,\n",
    "    # Losses\n",
    "    categorical_weight=1.0, continuous_weight=0.1, beta=5.0, weight_decay=1e-3,\n",
    "    # Categorical\n",
    "    smoothing=0.0, moving_average=True, similarity_regularization=0.0,\n",
    "    entropy_regularization=0.06\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Gaussian(1, 1) x Gaussian(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = product(\n",
    "    df1=create_1d_gaussian(mean=1.0, std=1.0, size=size),\n",
    "    df2=create_1d_gaussian(mean=-1.0, std=1.0, size=size)\n",
    ")\n",
    "df_synthesized, value_types = synthesize(\n",
    "    df_original=df_original, num_iterations=1000, num_logging=20,\n",
    "    # encoder/decoder\n",
    "    network_type='mlp', capacity=512, depth=2, layer_type='dense',\n",
    "    batchnorm=True, activation='relu', weight_decay=1e-3,\n",
    "    # encoding\n",
    "    encoding_type='variational', encoding_size=512, encoding_kwargs=dict(beta=0.0004),\n",
    "    # optimizer\n",
    "    optimizer='adam', learning_rate=3e-3, decay_steps=50, decay_rate=0.5,\n",
    "    clip_gradients=1.0, batch_size=128\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Bernoulli x Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (i) Uniform loss weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = product(\n",
    "    df1=create_bernoulli(probability=0.1, size=size),\n",
    "    df2=create_1d_gaussian(mean=0.0, std=1.0, size=size)\n",
    ")\n",
    "df_synthesized, value_types = synthesize(\n",
    "    df_original=df_original, num_iterations=1000, num_logging=20,\n",
    "    # encoder/decoder\n",
    "    network_type='mlp', capacity=512, depth=2, layer_type='dense',\n",
    "    batchnorm=True, activation='relu', weight_decay=1e-3,\n",
    "    # encoding\n",
    "    encoding_type='variational', encoding_size=512, encoding_kwargs=dict(beta=5.0),\n",
    "    # optimizer\n",
    "    optimizer='adam', learning_rate=1e-5, decay_steps=200, decay_rate=0.5,\n",
    "    clip_gradients=1.0, batch_size=128,\n",
    "    # losses\n",
    "    categorical_weight=1.0, continuous_weight=1.0,\n",
    "    # categorical\n",
    "    smoothing=0.0, moving_average=True, similarity_regularization=0.0,\n",
    "    entropy_regularization=0.05\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (ii) Biased loss weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = product(\n",
    "    df1=create_bernoulli(probability=0.1, size=size),\n",
    "    df2=create_1d_gaussian(mean=0.0, std=1.0, size=size)\n",
    ")\n",
    "df_synthesized, value_types = synthesize(\n",
    "    df_original=df_original, summarizer='summaries', num_iterations=1000, num_logging=20,\n",
    "    # encoder/decoder\n",
    "    network_type='mlp', capacity=512, depth=4, layer_type='dense',\n",
    "    batchnorm=True, activation='relu', weight_decay=0.0,\n",
    "    # encoding\n",
    "    encoding_type='variational', encoding_size=512, encoding_kwargs=dict(beta=2e-5),\n",
    "    # optimizer\n",
    "    optimizer='adam', learning_rate=3e-4, decay_steps=300, decay_rate=0.5,\n",
    "    clip_gradients=1.0, batch_size=512,\n",
    "    # losses\n",
    "    categorical_weight=1.0, continuous_weight=0.04,\n",
    "    # categorical\n",
    "    smoothing=0.0, moving_average=True, similarity_regularization=0.0,\n",
    "    entropy_regularization=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(df_original['x1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(df_synthesized['x1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df_original['x2'])\n",
    "sns.distplot(df_synthesized['x2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "synthesized",
   "language": "python",
   "name": "synthesized"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "stem_cell": {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
