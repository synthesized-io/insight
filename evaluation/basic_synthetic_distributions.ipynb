{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action='ignore', message='numpy.dtype size changed')\n",
    "warnings.filterwarnings(action='ignore', message='compiletime version 3.5 of module')\n",
    "\n",
    "if not 'workbookDir' in globals():\n",
    "    workbookDir = os.getcwd()\n",
    "os.chdir(os.path.split(workbookDir)[0])\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copyright (C) Synthesized Ltd. - All Rights Reserved\n",
      "License key: EE6B-6720-67A2-32F3-3139-2DF3-5D2D-B5F3\n",
      "Expires at: 2019-06-30 00:00:00\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import ks_2samp\n",
    "from synthesized.core import BasicSynthesizer\n",
    "from synthesized.testing.synthetic_distributions import *\n",
    "from synthesized.testing.synthetic_distributions import _plot_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_values(synthesizer):\n",
    "    print(' ', 'value types:')\n",
    "    for value in synthesizer.values:\n",
    "        print(' ', value.name, value)\n",
    "    print()\n",
    "\n",
    "def log_distance(df_original, df_synthesized):\n",
    "    distances = list()\n",
    "    for name in df_original:\n",
    "        distance = ks_2samp(df_original[name], df_synthesized[name]).statistic\n",
    "        suffix = ''\n",
    "        if distance < 0.05:\n",
    "            suffix += '*'\n",
    "        if distance < 0.01:\n",
    "            suffix += '*'\n",
    "        distances.append('{:.3f}{}'.format(distance, suffix))\n",
    "    print('/'.join(distances), end='  ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 10000\n",
    "\n",
    "def synthesize(df_original, num_iterations, num_logging, fn_logging=log_distance, **kwargs):\n",
    "    start = time.time()\n",
    "    with BasicSynthesizer(df_original, **kwargs) as synthesizer:\n",
    "        log_values(synthesizer)\n",
    "        value_types = {value.name: type(value) for value in synthesizer.values}\n",
    "        df_synthesized = synthesizer.synthesize(n=len(df_original))\n",
    "        fn_logging(df_original, df_synthesized)\n",
    "        for _ in range(num_iterations // num_logging):\n",
    "            synthesizer.learn(data=df_original, num_iterations=num_logging)\n",
    "            df_synthesized = synthesizer.synthesize(n=len(df_original))\n",
    "            assert len(df_synthesized) == len(df_original)\n",
    "            fn_logging(df_original, df_synthesized)\n",
    "        print()\n",
    "        print(' ', 'took', time.time() - start, 's')\n",
    "        return synthesizer.synthesize(n=len(df_original)), value_types\n",
    "\n",
    "def plot(data, synthesized, value_types):\n",
    "#     distances = [ks_2samp(data[col], synthesized[col])[0] for col in data.columns]\n",
    "#     avg_distance = np.mean(distances)\n",
    "#     evaluation[name + '_avg_distance'] = avg_distance\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5), sharex=True, sharey=True)\n",
    "    ax1.set_title('original')\n",
    "    ax2.set_title('synthesized')\n",
    "    _plot_data(data, ax=ax1, value_types=value_types)\n",
    "    _plot_data(synthesized, ax=ax2, value_types=value_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernoulli(0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "WARNING:tensorflow:From /home/ubuntu/.local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/ubuntu/.local/lib/python3.5/site-packages/tensorflow/python/ops/losses/losses_impl.py:209: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/ubuntu/.local/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "  value types:\n",
      "  x categorical2-281\n",
      "\n",
      "0.201  0.126  0.075  0.049*  0.045*  0.044*  0.040*  0.041*  0.040*  0.022*  0.034*  0.009**  0.009**  0.018*  0.012*  0.013*  0.020*  0.022*  0.014*  0.005**  0.005**  0.004**  0.017*  0.018*  0.015*  0.009**  0.016*  0.012*  0.010**  0.006**  0.007**  0.007**  0.011*  0.008**  0.002**  0.007**  0.004**  0.005**  0.005**  0.004**  0.002**  0.005**  0.001**  0.006**  0.001**  0.003**  0.000**  0.001**  0.002**  0.006**  0.001**  \n",
      "  took 42.76071763038635 s\n"
     ]
    }
   ],
   "source": [
    "df_original = create_bernoulli(probability=0.3, size=size)\n",
    "df_synthesized, value_types = synthesize(\n",
    "    df_original=df_original, summarizer=True, num_iterations=1000, num_logging=20,\n",
    "    # encoder/decoder\n",
    "    network_type='mlp', capacity=512, depth=2, layer_type='dense',\n",
    "    batchnorm=True, activation='relu', weight_decay=1e-3,\n",
    "    # encoding\n",
    "    encoding_type='variational', encoding_size=512, encoding_kwargs=dict(beta=5.0),\n",
    "    # optimizer\n",
    "    optimizer='adam', learning_rate=1e-5, decay_steps=200, decay_rate=0.5,\n",
    "    clip_gradients=1.0, batch_size=128,\n",
    "    # losses\n",
    "    categorical_weight=1.0, continuous_weight=1.0,\n",
    "    # categorical\n",
    "    smoothing=0.0, moving_average=True, similarity_regularization=0.0,\n",
    "    entropy_regularization=0.08\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Bernoulli(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% \n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "  value types:\n",
      "  x categorical2-281\n",
      "\n",
      "0.282  0.172  0.098  0.046*  0.011*  0.012*  0.002**  0.005**  0.004**  0.000**  0.001**  0.001**  0.006**  0.010**  0.008**  0.006**  0.007**  0.008**  0.006**  0.007**  0.002**  0.002**  0.002**  0.001**  0.001**  0.003**  0.001**  0.003**  0.000**  0.006**  0.004**  0.007**  0.001**  0.006**  0.001**  0.000**  0.002**  0.002**  0.000**  0.006**  0.001**  0.003**  0.001**  0.001**  0.001**  0.001**  0.002**  0.002**  0.005**  0.004**  0.004**  \n",
      "  took 42.84357261657715 s\n"
     ]
    }
   ],
   "source": [
    "df_original = create_bernoulli(probability=0.1, size=size)\n",
    "df_synthesized, value_types = synthesize(\n",
    "    df_original=df_original, summarizer=True, num_iterations=1000, num_logging=20,\n",
    "    # encoder/decoder\n",
    "    network_type='mlp', capacity=512, depth=2, layer_type='dense',\n",
    "    batchnorm=True, activation='relu', weight_decay=1e-3,\n",
    "    # encoding\n",
    "    encoding_type='variational', encoding_size=512, encoding_kwargs=dict(beta=5.0),\n",
    "    # optimizer\n",
    "    optimizer='adam', learning_rate=1e-5, decay_steps=200, decay_rate=0.5,\n",
    "    clip_gradients=1.0, batch_size=128,\n",
    "    # losses\n",
    "    categorical_weight=1.0, continuous_weight=1.0,\n",
    "    # categorical\n",
    "    smoothing=0.0, moving_average=True, similarity_regularization=0.0,\n",
    "    entropy_regularization=0.05\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "### Categorical([0.5, 0.25, 0.125, ...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "  value types:\n",
      "  x categorical5-458\n",
      "\n",
      "0.301  0.212  0.138  0.088  0.074  0.073  0.060  0.053  0.055  0.058  0.049*  0.049*  0.043*  0.034*  0.042*  0.036*  0.030*  0.036*  0.030*  0.024*  0.018*  0.021*  0.016*  0.016*  0.022*  0.027*  0.020*  0.029*  0.023*  0.023*  0.032*  0.028*  0.028*  0.032*  0.034*  0.024*  0.030*  0.032*  0.029*  0.028*  0.028*  0.029*  0.027*  0.020*  0.030*  0.027*  0.029*  0.032*  0.026*  0.033*  0.026*  \n",
      "  took 34.99243211746216 s\n"
     ]
    }
   ],
   "source": [
    "df_original = create_categorical(probabilities=[0.5, 0.25, 0.125, 0.0625, 0.0625], size=size)\n",
    "df_synthesized, value_types = synthesize(\n",
    "    df_original=df_original, summarizer=True, num_iterations=1000, num_logging=20,\n",
    "    # encoder/decoder\n",
    "    network_type='mlp', capacity=512, depth=2, layer_type='dense',\n",
    "    batchnorm=True, activation='relu', weight_decay=1e-3,\n",
    "    # encoding\n",
    "    encoding_type='variational', encoding_size=512, encoding_kwargs=dict(beta=5.0),\n",
    "    # optimizer\n",
    "    optimizer='adam', learning_rate=1e-5, decay_steps=200, decay_rate=0.5,\n",
    "    clip_gradients=1.0, batch_size=128,\n",
    "    # losses\n",
    "    categorical_weight=1.0, continuous_weight=1.0,\n",
    "    # categorical\n",
    "    smoothing=0.0, moving_average=True, similarity_regularization=0.0,\n",
    "    entropy_regularization=0.16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "### Gaussian(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "  value types:\n",
      "  x continuous-normal\n",
      "\n",
      "0.431  0.113  0.080  0.082  0.027*  0.027*  0.051  0.054  0.040*  0.029*  0.039*  0.044*  0.060  0.046*  0.041*  0.043*  0.050  0.050*  0.047*  0.040*  0.041*  0.038*  0.040*  0.039*  0.040*  0.041*  0.042*  0.040*  0.043*  0.039*  0.044*  0.044*  0.037*  0.042*  0.040*  0.040*  0.043*  0.044*  0.040*  0.043*  0.045*  0.042*  0.044*  0.040*  0.042*  0.041*  0.042*  0.042*  0.045*  0.044*  0.044*  \n",
      "  took 40.84885334968567 s\n"
     ]
    }
   ],
   "source": [
    "df_original = create_1d_gaussian(mean=0.0, std=1.0, size=size)\n",
    "df_synthesized, value_types = synthesize(\n",
    "    df_original=df_original, summarizer=True, num_iterations=1000, num_logging=20,\n",
    "    # encoder/decoder\n",
    "    network_type='mlp', capacity=512, depth=2, layer_type='dense',\n",
    "    batchnorm=True, activation='relu', weight_decay=1e-3,\n",
    "    # encoding\n",
    "    encoding_type='variational', encoding_size=512, encoding_kwargs=dict(beta=0.0004),\n",
    "    # optimizer\n",
    "    optimizer='adam', learning_rate=3e-3, decay_steps=50, decay_rate=0.5,\n",
    "    clip_gradients=1.0, batch_size=128,\n",
    "    # losses\n",
    "    categorical_weight=1.0, continuous_weight=1.0,\n",
    "    # categorical\n",
    "    smoothing=0.0, moving_average=True, similarity_regularization=0.0,\n",
    "    entropy_regularization=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian mixture N(0, 1) and N(3, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that fitting of distributions is not stable and for the same parameters of original distributuon we can get a different fitted distribution. We actually should not fit any distribution in this case. Shouldn't we decrease threshold for ditribution fitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "  value types:\n",
      "  x continuous-normal\n",
      "\n",
      "0.459  0.051  0.085  0.083  0.081  0.048*  0.084  0.075  0.069  0.037*  0.041*  \n",
      "  took 56.726388931274414 s\n"
     ]
    }
   ],
   "source": [
    "df_original = create_two_gaussian_mixtures(mean1=0.0, std1=1.0, mean2=3.0, std2=1.0, size=size)\n",
    "df_synthesized, value_types = synthesize(\n",
    "    df_original=df_original, summarizer=True, num_iterations=2000, num_logging=200,\n",
    "    # encoder/decoder\n",
    "    network_type='mlp', capacity=512, depth=2, layer_type='dense',\n",
    "    batchnorm=True, activation='relu', weight_decay=1e-5, # weight_decay=1e-5\n",
    "    # encoding\n",
    "    encoding_type='variational', encoding_size=512, encoding_kwargs=dict(beta=0.0005), # beta=0.0005\n",
    "    # optimizer\n",
    "    optimizer='adam', learning_rate=3e-3, decay_steps=360, decay_rate=1.1, # learning_rate=1e-4, decay_steps=200\n",
    "    clip_gradients=1.0, batch_size=128,\n",
    "    # losses\n",
    "    categorical_weight=1.0, continuous_weight=1.0,\n",
    "    # categorical\n",
    "    smoothing=0.0, moving_average=True, similarity_regularization=0.0,\n",
    "    entropy_regularization=0.0 # entropy_regularization=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernoulli(0.1) x Bernoulli(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "  value types:\n",
      "  x1 categorical2-281\n",
      "  x2 categorical2-281\n",
      "\n",
      "0.271/0.391  0.156/0.258  0.095/0.161  0.067/0.107  0.061/0.086  0.057/0.068  0.044*/0.050  0.038*/0.053  0.034*/0.043*  0.029*/0.036*  0.028*/0.033*  0.023*/0.035*  0.021*/0.036*  0.022*/0.036*  0.023*/0.028*  0.020*/0.026*  0.019*/0.021*  0.015*/0.016*  0.012*/0.020*  0.014*/0.020*  0.014*/0.016*  0.016*/0.016*  0.003**/0.012*  0.005**/0.011*  0.005**/0.008**  0.002**/0.010**  0.003**/0.006**  0.000**/0.011*  0.001**/0.007**  0.002**/0.005**  0.007**/0.009**  0.001**/0.000**  0.006**/0.003**  0.002**/0.001**  0.001**/0.002**  0.008**/0.000**  0.002**/0.005**  0.005**/0.001**  0.005**/0.005**  0.006**/0.001**  0.005**/0.003**  0.011*/0.003**  0.006**/0.004**  0.005**/0.002**  0.007**/0.001**  0.010*/0.004**  0.008**/0.001**  0.007**/0.002**  0.010*/0.003**  0.007**/0.000**  0.008**/0.002**  \n",
      "  took 35.69032692909241 s\n"
     ]
    }
   ],
   "source": [
    "df_original = product(\n",
    "    df1=create_bernoulli(probability=0.1, size=size),\n",
    "    df2=create_bernoulli(probability=0.1, size=size)\n",
    ")\n",
    "df_synthesized, value_types = synthesize(\n",
    "    df_original=df_original, summarizer=True, num_iterations=1000, num_logging=20,\n",
    "    # encoder/decoder\n",
    "    network_type='mlp', capacity=512, depth=2, layer_type='dense',\n",
    "    batchnorm=True, activation='relu', weight_decay=1e-3,\n",
    "    # encoding\n",
    "    encoding_type='variational', encoding_size=512, encoding_kwargs=dict(beta=5.0),\n",
    "    # optimizer\n",
    "    optimizer='adam', learning_rate=1e-5, decay_steps=200, decay_rate=0.5,\n",
    "    clip_gradients=1.0, batch_size=128,\n",
    "    # losses\n",
    "    categorical_weight=1.0, continuous_weight=1.0,\n",
    "    # categorical\n",
    "    smoothing=0.0, moving_average=True, similarity_regularization=0.0,\n",
    "    entropy_regularization=0.07\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian(1, 1) x Gaussian(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "  value types:\n",
      "  x1 continuous-weibull\n",
      "  x2 continuous-gamma\n",
      "\n",
      "0.414/0.426  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.5/site-packages/pandas/core/indexing.py:635: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n",
      "/home/ubuntu/.local/lib/python3.5/site-packages/pandas/core/indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.073/0.076  0.024*/0.018*  0.016*/0.045*  0.042*/0.038*  0.033*/0.052  0.043*/0.027*  0.050*/0.039*  0.026*/0.036*  0.030*/0.031*  0.052/0.040*  0.032*/0.039*  0.037*/0.053  0.039*/0.059  0.046*/0.048*  0.042*/0.046*  0.041*/0.041*  0.038*/0.042*  0.040*/0.041*  0.039*/0.038*  0.043*/0.039*  0.044*/0.040*  0.043*/0.045*  0.045*/0.045*  0.041*/0.048*  0.039*/0.045*  0.040*/0.048*  0.044*/0.046*  0.038*/0.043*  0.044*/0.045*  0.042*/0.049*  0.042*/0.043*  0.038*/0.042*  0.039*/0.040*  0.040*/0.044*  0.039*/0.042*  0.041*/0.045*  0.037*/0.042*  0.038*/0.042*  0.038*/0.044*  0.038*/0.048*  0.040*/0.049*  0.037*/0.040*  0.040*/0.042*  0.040*/0.043*  0.040*/0.045*  0.041*/0.045*  0.041*/0.041*  0.039*/0.046*  0.040*/0.047*  0.039*/0.041*  \n",
      "  took 65.91362929344177 s\n"
     ]
    }
   ],
   "source": [
    "df_original = product(\n",
    "    df1=create_1d_gaussian(mean=1.0, std=1.0, size=size),\n",
    "    df2=create_1d_gaussian(mean=-1.0, std=1.0, size=size)\n",
    ")\n",
    "df_synthesized, value_types = synthesize(\n",
    "    df_original=df_original, summarizer=True, num_iterations=1000, num_logging=20,\n",
    "    # encoder/decoder\n",
    "    network_type='mlp', capacity=512, depth=2, layer_type='dense',\n",
    "    batchnorm=True, activation='relu', weight_decay=1e-3,\n",
    "    # encoding\n",
    "    encoding_type='variational', encoding_size=512, encoding_kwargs=dict(beta=0.0004),\n",
    "    # optimizer\n",
    "    optimizer='adam', learning_rate=3e-3, decay_steps=50, decay_rate=0.5,\n",
    "    clip_gradients=1.0, batch_size=128,\n",
    "    # losses\n",
    "    categorical_weight=1.0, continuous_weight=1.0,\n",
    "    # categorical\n",
    "    smoothing=0.0, moving_average=True, similarity_regularization=0.0,\n",
    "    entropy_regularization=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernoulli x Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "  value types:\n",
      "  x1 categorical2-281\n",
      "  x2 continuous-normal\n",
      "\n",
      "0.767/0.411  0.651/0.410  0.533/0.408  0.394/0.406  0.282/0.405  0.200/0.405  0.137/0.404  0.089/0.406  0.058/0.405  0.040*/0.406  0.018*/0.404  0.009**/0.406  0.005**/0.404  0.004**/0.406  0.009**/0.406  0.012*/0.403  0.015*/0.407  0.020*/0.404  0.024*/0.406  0.027*/0.405  0.027*/0.405  0.023*/0.405  0.027*/0.405  0.027*/0.402  0.031*/0.404  0.032*/0.407  0.028*/0.406  0.031*/0.405  0.029*/0.405  0.034*/0.404  0.034*/0.404  0.035*/0.403  0.031*/0.405  0.035*/0.407  0.037*/0.406  0.034*/0.406  0.037*/0.406  0.036*/0.405  0.033*/0.403  0.036*/0.406  0.035*/0.406  0.039*/0.405  0.035*/0.404  0.036*/0.406  0.035*/0.407  0.035*/0.406  0.034*/0.406  0.038*/0.407  0.038*/0.404  0.039*/0.405  0.038*/0.404  \n",
      "  took 32.62858510017395 s\n"
     ]
    }
   ],
   "source": [
    "df_original = product(\n",
    "    df1=create_bernoulli(probability=0.1, size=size),\n",
    "    df2=create_1d_gaussian(mean=0.0, std=1.0, size=size)\n",
    ")\n",
    "df_synthesized, value_types = synthesize(\n",
    "    df_original=df_original, summarizer=True, num_iterations=1000, num_logging=20,\n",
    "    # encoder/decoder\n",
    "    network_type='mlp', capacity=512, depth=2, layer_type='dense',\n",
    "    batchnorm=True, activation='relu', weight_decay=1e-3,\n",
    "    # encoding\n",
    "    encoding_type='variational', encoding_size=512, encoding_kwargs=dict(beta=5.0),\n",
    "    # optimizer\n",
    "    optimizer='adam', learning_rate=1e-5, decay_steps=200, decay_rate=0.5,\n",
    "    clip_gradients=1.0, batch_size=128,\n",
    "    # losses\n",
    "    categorical_weight=1.0, continuous_weight=1.0,\n",
    "    # categorical\n",
    "    smoothing=0.0, moving_average=True, similarity_regularization=0.0,\n",
    "    entropy_regularization=0.05\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "stem_cell": {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
