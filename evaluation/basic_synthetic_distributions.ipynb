{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action='ignore', message='numpy.dtype size changed')\n",
    "warnings.filterwarnings(action='ignore', message='compiletime version 3.5 of module')\n",
    "\n",
    "if not 'workbookDir' in globals():\n",
    "    workbookDir = os.getcwd()\n",
    "os.chdir(os.path.split(workbookDir)[0])\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import ks_2samp\n",
    "import seaborn as sns\n",
    "from synthesized import BasicSynthesizer\n",
    "from synthesized.testing.synthetic_distributions import *\n",
    "from synthesized.testing.synthetic_distributions import _plot_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_values(synthesizer):\n",
    "    print(' ', 'value types:')\n",
    "    for value in synthesizer.values:\n",
    "        print(' ', value.name, value)\n",
    "    print()\n",
    "\n",
    "def log_distance(df_original, df_synthesized):\n",
    "    distances = list()\n",
    "    for name in df_original:\n",
    "        distance = ks_2samp(df_original[name], df_synthesized[name]).statistic\n",
    "        suffix = ''\n",
    "        if distance < 0.05:\n",
    "            suffix += '*'\n",
    "        if distance < 0.01:\n",
    "            suffix += '*'\n",
    "        distances.append('{:.3f}{}'.format(distance, suffix))\n",
    "    print('/'.join(distances), end='  ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 100000\n",
    "\n",
    "def synthesize(df_original, num_iterations, num_logging, fn_logging=log_distance, **kwargs):\n",
    "    start = time.time()\n",
    "    with BasicSynthesizer(df_original, **kwargs) as synthesizer:\n",
    "        log_values(synthesizer)\n",
    "        value_types = {value.name: type(value) for value in synthesizer.values}\n",
    "        df_synthesized = synthesizer.synthesize(num_rows=len(df_original))\n",
    "        fn_logging(df_original, df_synthesized)\n",
    "        for _ in range(num_iterations // num_logging):\n",
    "            synthesizer.learn(data=df_original, num_iterations=num_logging)\n",
    "            df_synthesized = synthesizer.synthesize(num_rows=len(df_original))\n",
    "            assert len(df_synthesized) == len(df_original)\n",
    "            fn_logging(df_original, df_synthesized)\n",
    "        print()\n",
    "        print(' ', 'took', time.time() - start, 's')\n",
    "        return synthesizer.synthesize(num_rows=len(df_original)), value_types\n",
    "\n",
    "def plot(data, synthesized, value_types):\n",
    "#     distances = [ks_2samp(data[col], synthesized[col])[0] for col in data.columns]\n",
    "#     avg_distance = np.mean(distances)\n",
    "#     evaluation[name + '_avg_distance'] = avg_distance\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5), sharex=True, sharey=True)\n",
    "    ax1.set_title('original')\n",
    "    ax2.set_title('synthesized')\n",
    "    _plot_data(data, ax=ax1, value_types=value_types)\n",
    "    _plot_data(synthesized, ax=ax2, value_types=value_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Single-column categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting hyperparameters: entropy regularization, learning rate and decay, beta loss.\n",
    "\n",
    "Only change between different categorical distributions: entropy regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Bernoulli(0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/alexkuhnle/PycharmProjects/synthesized/venv/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/alexkuhnle/PycharmProjects/synthesized/venv/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "  value types:\n",
      "  x categorical2-140\n",
      "\n",
      "0.004**  0.038*  0.075  0.028*  0.054  0.058  0.054  0.094  0.095  0.063  0.073  0.075  0.073  0.050  0.087  0.110  0.082  0.075  0.073  0.074  0.088  0.103  0.079  0.082  0.094  0.088  0.081  0.073  0.086  0.084  0.081  0.087  0.093  0.095  0.090  0.087  0.084  0.089  0.101  0.097  0.092  0.094  0.087  0.088  0.082  0.077  0.076  0.076  0.074  0.078  0.080  \n",
      "  took 81.26215505599976 s\n"
     ]
    }
   ],
   "source": [
    "df_original = create_bernoulli(probability=0.3, size=size)\n",
    "df_synthesized, value_types = synthesize(\n",
    "    df_original=df_original, summarizer='summaries', num_iterations=1000, num_logging=20,\n",
    "    # VAE distribution\n",
    "    distribution='normal', latent_size=512,\n",
    "    # Network\n",
    "    network='mlp', capacity=256, depth=4, batchnorm=True, activation='relu',\n",
    "    # Optimizer\n",
    "    optimizer='adam', learning_rate=1e-3, decay_steps=300, decay_rate=0.5,\n",
    "    initial_boost=True, clip_gradients=1.0, batch_size=128,\n",
    "    # Losses\n",
    "    categorical_weight=10.0, continuous_weight=0.3, beta=1e-2, weight_decay=0.0,\n",
    "    # Categorical\n",
    "    temperature=0.15, smoothing=0.0, moving_average=False,\n",
    "    similarity_regularization=0.0, entropy_regularization=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "### (b) Bernoulli(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% \n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  value types:\n",
      "  x categorical2-140\n",
      "\n",
      "0.622  0.069  0.018*  0.023*  0.022*  0.020*  0.028*  0.033*  0.028*  0.024*  0.042*  0.045*  0.041*  0.049*  0.042*  0.044*  0.038*  0.054  0.045*  0.045*  0.043*  0.034*  0.043*  0.064  0.052  0.034*  0.031*  0.051  0.048*  0.055  0.039*  0.052  0.054  0.053  0.044*  0.039*  0.047*  0.051  0.049*  0.041*  0.055  0.057  0.055  0.053  0.048*  0.057  0.054  0.052  0.050  0.049*  0.044*  \n",
      "  took 78.25076007843018 s\n"
     ]
    }
   ],
   "source": [
    "df_original = create_bernoulli(probability=0.1, size=size)\n",
    "df_synthesized, value_types = synthesize(\n",
    "    df_original=df_original, summarizer='summaries', num_iterations=1000, num_logging=20,\n",
    "    # VAE distribution\n",
    "    distribution='normal', latent_size=512,\n",
    "    # Network\n",
    "    network='mlp', capacity=256, depth=4, batchnorm=True, activation='relu',\n",
    "    # Optimizer\n",
    "    optimizer='adam', learning_rate=1e-3, decay_steps=300, decay_rate=0.5,\n",
    "    initial_boost=True, clip_gradients=1.0, batch_size=128,\n",
    "    # Losses\n",
    "    categorical_weight=10.0, continuous_weight=0.3, beta=1e-2, weight_decay=0.0,\n",
    "    # Categorical\n",
    "    temperature=0.15, smoothing=0.0, moving_average=False,\n",
    "    similarity_regularization=0.0, entropy_regularization=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "### (c) Categorical([0.5, 0.25, 0.125, ...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  value types:\n",
      "  x categorical5-229\n",
      "\n",
      "0.423  0.033*  0.069  0.061  0.070  0.072  0.021*  0.060  0.045*  0.065  0.050  0.029*  0.062  0.083  0.061  0.035*  0.032*  0.059  0.068  0.077  0.051  0.047*  0.061  0.072  0.075  0.084  0.077  0.062  0.058  0.057  0.048*  0.056  0.056  0.053  0.063  0.055  0.056  0.053  0.063  0.066  0.068  0.073  0.076  0.071  0.066  0.061  0.063  0.060  0.060  0.057  0.056  \n",
      "  took 75.52380204200745 s\n"
     ]
    }
   ],
   "source": [
    "df_original = create_categorical(probabilities=[0.5, 0.25, 0.125, 0.0625, 0.0625], size=size)\n",
    "df_synthesized, value_types = synthesize(\n",
    "    df_original=df_original, summarizer='summaries', num_iterations=1000, num_logging=20,\n",
    "    # VAE distribution\n",
    "    distribution='normal', latent_size=512,\n",
    "    # Network\n",
    "    network='mlp', capacity=256, depth=4, batchnorm=True, activation='relu',\n",
    "    # Optimizer\n",
    "    optimizer='adam', learning_rate=1e-3, decay_steps=300, decay_rate=0.5,\n",
    "    initial_boost=True, clip_gradients=1.0, batch_size=128,\n",
    "    # Losses\n",
    "    categorical_weight=10.0, continuous_weight=0.3, beta=1e-2, weight_decay=0.0,\n",
    "    # Categorical\n",
    "    temperature=0.15, smoothing=0.0, moving_average=False,\n",
    "    similarity_regularization=0.0, entropy_regularization=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Single-column continuous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting hyperparameters: learning rate and decay, beta loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "### (a) Gaussian(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  value types:\n",
      "  x continuous-weibull\n",
      "\n",
      "0.227  0.067  0.074  0.060  0.034*  0.063  0.037*  0.023*  0.043*  0.033*  0.055  0.043*  0.040*  0.053  0.047*  0.038*  0.026*  0.040*  0.035*  0.053  0.029*  0.039*  0.029*  0.051  0.050*  0.038*  0.032*  0.042*  0.029*  0.042*  0.035*  0.024*  0.046*  0.035*  0.035*  0.036*  0.036*  0.037*  0.040*  0.036*  0.034*  0.029*  0.040*  0.031*  0.035*  0.036*  0.044*  0.032*  0.029*  0.037*  0.043*  \n",
      "  took 77.08820104598999 s\n"
     ]
    }
   ],
   "source": [
    "df_original = create_1d_gaussian(mean=0.0, std=1.0, size=size)\n",
    "df_synthesized, value_types = synthesize(\n",
    "    df_original=df_original, summarizer='summaries', num_iterations=1000, num_logging=20,\n",
    "    # VAE distribution\n",
    "    distribution='normal', latent_size=512,\n",
    "    # Network\n",
    "    network='mlp', capacity=256, depth=4, batchnorm=True, activation='relu',\n",
    "    # Optimizer\n",
    "    optimizer='adam', learning_rate=1e-3, decay_steps=300, decay_rate=0.5,\n",
    "    initial_boost=True, clip_gradients=1.0, batch_size=128,\n",
    "    # Losses\n",
    "    categorical_weight=10.0, continuous_weight=0.3, beta=1e-2, weight_decay=0.0,\n",
    "    # Categorical\n",
    "    temperature=0.15, smoothing=0.0, moving_average=False,\n",
    "    similarity_regularization=0.0, entropy_regularization=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Gaussian mixture N(0, 1) and N(3, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that fitting of distributions is not stable and for the same parameters of original distribution we can get a different fitted distribution. We actually should not fit any distribution in this case. Shouldn't we decrease threshold for distribution fitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  value types:\n",
      "  x continuous-normal\n",
      "\n",
      "0.367  0.133  0.097  0.117  0.110  0.109  0.087  0.085  0.084  0.069  0.082  0.071  0.086  0.076  0.067  0.075  0.072  0.080  0.069  0.072  0.065  0.078  0.068  0.072  0.080  0.069  0.072  0.064  0.044*  0.085  0.079  0.075  0.057  0.060  0.059  0.055  0.063  0.066  0.062  0.076  0.072  0.070  0.075  0.069  0.068  0.059  0.071  0.070  0.052  0.058  0.056  \n",
      "  took 75.97708010673523 s\n"
     ]
    }
   ],
   "source": [
    "df_original = create_two_gaussian_mixtures(mean1=0.0, std1=1.0, mean2=3.0, std2=1.0, size=size)\n",
    "df_synthesized, value_types = synthesize(\n",
    "    df_original=df_original, summarizer='summaries', num_iterations=1000, num_logging=20,\n",
    "    # VAE distribution\n",
    "    distribution='normal', latent_size=512,\n",
    "    # Network\n",
    "    network='mlp', capacity=256, depth=4, batchnorm=True, activation='relu',\n",
    "    # Optimizer\n",
    "    optimizer='adam', learning_rate=1e-3, decay_steps=300, decay_rate=0.5,\n",
    "    initial_boost=True, clip_gradients=1.0, batch_size=128,\n",
    "    # Losses\n",
    "    categorical_weight=10.0, continuous_weight=0.3, beta=1e-2, weight_decay=0.0,\n",
    "    # Categorical\n",
    "    temperature=0.15, smoothing=0.0, moving_average=False,\n",
    "    similarity_regularization=0.0, entropy_regularization=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (i) Production-like settings which sould produce reasonable result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = create_two_gaussian_mixtures(mean1=0.0, std1=1.0, mean2=3.0, std2=1.0, size=size)\n",
    "df_synthesized, value_types = synthesize(\n",
    "    df_original=df_original, summarizer=None, num_iterations=5000, num_logging=100,\n",
    "    # VAE distribution\n",
    "    distribution='normal', latent_size=512,\n",
    "    # Network\n",
    "    network='mlp', capacity=128, depth=2, batchnorm=True, activation='relu',\n",
    "    # Optimizer\n",
    "    optimizer='adam', learning_rate=3e-4, decay_steps=1000, decay_rate=0.9,\n",
    "    clip_gradients=1.0, batch_size=64,\n",
    "    # Losses\n",
    "    categorical_weight=1.0, continuous_weight=0.1, beta=0.001, weight_decay=1e-5,\n",
    "    # Categorical\n",
    "    smoothing=0.0, moving_average=True, similarity_regularization=0.0,\n",
    "    entropy_regularization=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original.hist(bins=100)\n",
    "df_synthesized.hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Two-column product distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Bernoulli(0.1) x Bernoulli(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same hyperparameters as for single-column case, but slightly increased entropy regularization value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  value types:\n",
      "  x1 categorical2-140\n",
      "  x2 categorical2-140\n",
      "\n",
      "0.817/0.059  0.040*/0.006**  0.047*/0.028*  0.012*/0.049*  0.028*/0.012*  0.047*/0.023*  0.030*/0.002**  0.024*/0.027*  0.062/0.009**  0.047*/0.011*  0.042*/0.010**  0.059/0.023*  0.062/0.018*  0.044*/0.001**  0.043*/0.008**  0.033*/0.001**  0.028*/0.001**  0.029*/0.017*  0.034*/0.003**  0.046*/0.012*  0.047*/0.015*  0.033*/0.006**  0.041*/0.006**  0.033*/0.003**  0.050*/0.006**  0.059/0.007**  0.044*/0.012*  0.037*/0.001**  0.048*/0.017*  0.051/0.017*  0.040*/0.005**  0.050*/0.002**  0.047*/0.006**  0.047*/0.002**  0.053/0.006**  0.053/0.000**  0.047*/0.006**  0.043*/0.016*  0.039*/0.011*  0.039*/0.006**  0.040*/0.002**  0.042*/0.003**  0.037*/0.004**  0.035*/0.004**  0.028*/0.002**  0.039*/0.001**  0.047*/0.002**  0.045*/0.007**  0.044*/0.003**  0.040*/0.001**  0.042*/0.002**  \n",
      "  took 93.68865990638733 s\n"
     ]
    }
   ],
   "source": [
    "df_original = product(\n",
    "    df1=create_bernoulli(probability=0.1, size=size),\n",
    "    df2=create_bernoulli(probability=0.1, size=size)\n",
    ")\n",
    "df_synthesized, value_types = synthesize(\n",
    "    df_original=df_original, summarizer='summaries', num_iterations=1000, num_logging=20,\n",
    "    # VAE distribution\n",
    "    distribution='normal', latent_size=512,\n",
    "    # Network\n",
    "    network='mlp', capacity=256, depth=4, batchnorm=True, activation='relu',\n",
    "    # Optimizer\n",
    "    optimizer='adam', learning_rate=1e-3, decay_steps=300, decay_rate=0.5,\n",
    "    initial_boost=True, clip_gradients=1.0, batch_size=128,\n",
    "    # Losses\n",
    "    categorical_weight=10.0, continuous_weight=0.3, beta=1e-2, weight_decay=0.0,\n",
    "    # Categorical\n",
    "    temperature=0.15, smoothing=0.0, moving_average=False,\n",
    "    similarity_regularization=0.0, entropy_regularization=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Gaussian(1, 1) x Gaussian(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = product(\n",
    "    df1=create_1d_gaussian(mean=1.0, std=1.0, size=size),\n",
    "    df2=create_1d_gaussian(mean=-1.0, std=1.0, size=size)\n",
    ")\n",
    "df_synthesized, value_types = synthesize(\n",
    "    df_original=df_original, num_iterations=1000, num_logging=20,\n",
    "    # encoder/decoder\n",
    "    network_type='mlp', capacity=512, depth=2, layer_type='dense',\n",
    "    batchnorm=True, activation='relu', weight_decay=1e-3,\n",
    "    # encoding\n",
    "    encoding_type='variational', encoding_size=512, encoding_kwargs=dict(beta=0.0004),\n",
    "    # optimizer\n",
    "    optimizer='adam', learning_rate=3e-3, decay_steps=50, decay_rate=0.5,\n",
    "    clip_gradients=1.0, batch_size=128\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Bernoulli x Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (i) Uniform loss weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = product(\n",
    "    df1=create_bernoulli(probability=0.1, size=size),\n",
    "    df2=create_1d_gaussian(mean=0.0, std=1.0, size=size)\n",
    ")\n",
    "df_synthesized, value_types = synthesize(\n",
    "    df_original=df_original, num_iterations=1000, num_logging=20,\n",
    "    # encoder/decoder\n",
    "    network_type='mlp', capacity=512, depth=2, layer_type='dense',\n",
    "    batchnorm=True, activation='relu', weight_decay=1e-3,\n",
    "    # encoding\n",
    "    encoding_type='variational', encoding_size=512, encoding_kwargs=dict(beta=5.0),\n",
    "    # optimizer\n",
    "    optimizer='adam', learning_rate=1e-5, decay_steps=200, decay_rate=0.5,\n",
    "    clip_gradients=1.0, batch_size=128,\n",
    "    # losses\n",
    "    categorical_weight=1.0, continuous_weight=1.0,\n",
    "    # categorical\n",
    "    smoothing=0.0, moving_average=True, similarity_regularization=0.0,\n",
    "    entropy_regularization=0.05\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (ii) Biased loss weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = product(\n",
    "    df1=create_bernoulli(probability=0.1, size=size),\n",
    "    df2=create_1d_gaussian(mean=0.0, std=1.0, size=size)\n",
    ")\n",
    "df_synthesized, value_types = synthesize(\n",
    "    df_original=df_original, summarizer='summaries', num_iterations=1000, num_logging=20,\n",
    "    # encoder/decoder\n",
    "    network_type='mlp', capacity=512, depth=4, layer_type='dense',\n",
    "    batchnorm=True, activation='relu', weight_decay=0.0,\n",
    "    # encoding\n",
    "    encoding_type='variational', encoding_size=512, encoding_kwargs=dict(beta=2e-5),\n",
    "    # optimizer\n",
    "    optimizer='adam', learning_rate=3e-4, decay_steps=300, decay_rate=0.5,\n",
    "    clip_gradients=1.0, batch_size=512,\n",
    "    # losses\n",
    "    categorical_weight=1.0, continuous_weight=0.04,\n",
    "    # categorical\n",
    "    smoothing=0.0, moving_average=True, similarity_regularization=0.0,\n",
    "    entropy_regularization=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(df_original['x1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(df_synthesized['x1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df_original['x2'])\n",
    "sns.distplot(df_synthesized['x2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "synthesized",
   "language": "python",
   "name": "synthesized"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "stem_cell": {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
