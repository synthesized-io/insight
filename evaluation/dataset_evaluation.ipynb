{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "! jt -t grade3 -nf opensans"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "\u003ch1\u003e\u003ccenter\u003e1. Load Data\u003c/center\u003e\u003c/h1\u003e\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": "import os\nimport json\nimport warnings\nimport pandas as pd\n\nwarnings.filterwarnings(action\u003d\u0027ignore\u0027, message\u003d\u0027numpy.dtype size changed\u0027)\nwarnings.filterwarnings(action\u003d\u0027ignore\u0027, message\u003d\u0027compiletime version 3.5 of module\u0027)\n\nif not \u0027workbookDir\u0027 in globals():\n    workbookDir \u003d os.getcwd()\nos.chdir(os.path.split(workbookDir)[0])\n\n%load_ext autoreload\n%autoreload 2\n%matplotlib inline"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": "from synthesized.testing.evaluation import Evaluation\n#evaluation \u003d Evaluation(config_path\u003d\u0027configs/evaluation/dataset_evaluation.json\u0027, name\u003d\u0027james\u0027) # use this line if you want to run this notbook manually\nevaluation_name \u003d os.environ.get(\u0027EVALUATION_NAME\u0027, \u0027n/a\u0027)\nbranch \u003d os.environ.get(\u0027EVALUATION_BRANCH\u0027, \u0027n/a\u0027)\nrevision \u003d os.environ.get(\u0027EVALUATION_REVISION\u0027, \u0027n/a\u0027)\nevaluation \u003d Evaluation(branch\u003dbranch, revision\u003drevision, group\u003d\"dataset_evaluation\")\n\nconfig_path \u003d os.environ.get(\u0027EVALUATION_CONFIG_PATH\u0027, \u0027n/a\u0027)\nwith open(config_path, \u0027r\u0027) as f:\n    configs \u003d json.load(f)\n    evaluation.record_config(evaluation\u003devaluation_name, config\u003dconfigs[\"instances\"][evaluation_name])\n        "
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": "data \u003d pd.read_csv(evaluation.configs[evaluation_name][\u0027data\u0027])\ndata \u003d data.drop(evaluation.configs[evaluation_name][\u0027ignore_columns\u0027], axis\u003d1)\ndata.dropna(inplace\u003dTrue)\ndata.head(5)"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\u003ch1\u003e\u003ccenter\u003e2. Train model and generate synthetic data\u003c/center\u003e\u003c/h1\u003e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {},
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import synthesized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "train, test \u003d train_test_split(data, test_size\u003d0.2, random_state\u003d0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": "def train_and_synthesize(synthesizer_class\u003devaluation.configs[evaluation_name][\"synthesizer_class\"]):\n    assert synthesizer_class in {\"HighDimSynthesizer\", \"SeriesSynthesizer\"}\n    synthesizer_constructor \u003d getattr(synthesized, synthesizer_class)\n    loss_history \u003d list()\n    \n    def callback(synth, iteration, losses):\n        if len(loss_history) \u003d\u003d 0:\n            loss_history.append(losses)\n        else:\n            loss_history.append({name: losses[name] for name in loss_history[0]})\n    \n    with synthesizer_constructor(df\u003ddata, **evaluation.configs[evaluation_name][\u0027params\u0027]) as synthesizer:\n        synthesizer.learn(\n            df_train\u003dtrain, num_iterations\u003devaluation.configs[evaluation_name][\u0027num_iterations\u0027],\n            callback\u003dcallback, callback_freq\u003d100\n        )\n        if synthesizer_class \u003d\u003d \"HighDimSynthesizer\":\n            synthesized_data \u003d synthesizer.synthesize(num_rows\u003dlen(test))\n        else:\n            series_lengths \u003d data.groupby(evaluation.configs[evaluation_name][\"params\"][\"identifier_label\"]).count().to_numpy().transpose()\n            series_lengths \u003d list(series_lengths[0])\n            synthesized_data \u003d synthesizer.synthesize(series_lengths\u003dseries_lengths)\n        return synthesizer, synthesized_data, loss_history"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {},
        "scrolled": true
      },
      "outputs": [],
      "source": "%%capture\nsynthesizers_and_results \u003d [train_and_synthesize() for i in range(evaluation.configs[evaluation_name][\u0027num_passes\u0027])]\nsynthesizer \u003d synthesizers_and_results[0][0]"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## Plot losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "loss_history \u003d synthesizers_and_results[0][2]\n",
        "pd.DataFrame.from_records(loss_history).plot(figsize\u003d(15,7))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## Display aggregated statistics "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "from scipy.stats import ks_2samp\n",
        "import numpy as np\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": "def plot_avg_distances():\n    result \u003d []\n    for i, (synthesizer, synthesized_, _) in enumerate(synthesizers_and_results):\n        test_ \u003d synthesizer.preprocess(test)\n        synthesized_ \u003d synthesizer.preprocess(synthesized_)\n        distances \u003d [ks_2samp(test_[col], synthesized_[col])[0] for col in synthesized_.columns]\n        avg_distance \u003d np.mean(distances)\n        print(\u0027run: {}, AVG distance: {}\u0027.format(i+1, avg_distance))\n        result.append({\u0027run\u0027: i+1, \u0027avg_distance\u0027: avg_distance})\n        evaluation.record_metric(evaluation\u003devaluation_name, key\u003d\u0027avg_distance\u0027, value\u003davg_distance)\n    df \u003d pd.DataFrame.from_records(result)\n    df[\u0027run\u0027] \u003d df[\u0027run\u0027].astype(\u0027category\u0027)\n    g \u003d sns.barplot(y\u003d\u0027run\u0027, x\u003d\u0027avg_distance\u0027, data\u003ddf)\n    g.set_xlim(0.0, 1.0)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "plot_avg_distances()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## Details for the 1st Run "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "from synthesized.testing import UtilityTesting\n",
        "testing \u003d UtilityTesting(synthesizer, train, test, synthesizers_and_results[0][1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "testing.show_distribution_distances()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "testing.show_distributions(remove_outliers\u003d0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## Display correlations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "testing.show_corr_distances()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "testing.show_corr_matrices()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## Demonstrate the utility for training ML models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": "try:\n    utility \u003d testing.utility(target\u003devaluation.configs[evaluation_name][\u0027target\u0027])\nexcept:\n    utility \u003d 0.0\nevaluation.record_metric(evaluation\u003devaluation_name, key\u003d\u0027utility\u0027, value\u003dutility)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "evaluation.write_metrics()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}