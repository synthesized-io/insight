{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "! jt -t grade3 -nf opensans"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "\u003ch1\u003e\u003ccenter\u003e1. Load Data\u003c/center\u003e\u003c/h1\u003e\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import warnings\n",
        "import pandas as pd\n",
        "\n",
        "warnings.filterwarnings(action\u003d\u0027ignore\u0027, message\u003d\u0027numpy.dtype size changed\u0027)\n",
        "warnings.filterwarnings(action\u003d\u0027ignore\u0027, message\u003d\u0027compiletime version 3.5 of module\u0027)\n",
        "\n",
        "if not \u0027workbookDir\u0027 in globals():\n",
        "    workbookDir \u003d os.getcwd()\n",
        "os.chdir(os.path.split(workbookDir)[0])\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": "from synthesized.testing.evaluation import Evaluation\nfrom synthesized.testing import plotting as syn_plot\n#evaluation \u003d Evaluation(config_path\u003d\u0027configs/evaluation/dataset_evaluation.json\u0027, name\u003d\u0027james\u0027) # use this line if you want to run this notbook manually\nevaluation_name \u003d os.environ.get(\u0027EVALUATION_NAME\u0027, \u0027n/a\u0027)\nbranch \u003d os.environ.get(\u0027EVALUATION_BRANCH\u0027, \u0027n/a\u0027)\nrevision \u003d os.environ.get(\u0027EVALUATION_REVISION\u0027, \u0027n/a\u0027)\nevaluation \u003d Evaluation(branch\u003dbranch, revision\u003drevision, group\u003d\"dataset_evaluation\")\n\nconfig_path \u003d os.environ.get(\u0027EVALUATION_CONFIG_PATH\u0027, \u0027n/a\u0027)\nwith open(config_path, \u0027r\u0027) as f:\n    configs \u003d json.load(f)\n    config \u003d configs[\"instances\"][evaluation_name]\n    evaluation.record_config(evaluation\u003devaluation_name, config\u003dconfig)\n        "
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "data \u003d pd.read_csv(evaluation.configs[evaluation_name][\u0027data\u0027])\n",
        "data \u003d data.drop(evaluation.configs[evaluation_name][\u0027ignore_columns\u0027], axis\u003d1)\n",
        "data.dropna(inplace\u003dTrue)\n",
        "data.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\u003ch1\u003e\u003ccenter\u003e2. Train model and generate synthetic data\u003c/center\u003e\u003c/h1\u003e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {},
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "train, test \u003d train_test_split(data, test_size\u003d0.2, random_state\u003d0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {},
        "scrolled": true
      },
      "outputs": [],
      "source": "testing \u003d syn_plot.synthesize_and_plot(data\u003ddata, name\u003devaluation_name, evaluation\u003devaluation, config\u003dconfig, \n                                       metrics\u003d{}, test_data\u003dtest, plot_losses\u003dTrue, plot_distances\u003dTrue,\n                                       show_distribution_distances\u003dTrue, show_distributions\u003dTrue,\n                                       show_correlation_distances\u003dTrue, show_correlation_matrix\u003dTrue)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "try:\n    utility \u003d testing.utility(target\u003devaluation.configs[evaluation_name][\u0027target\u0027])\nexcept:\n    utility \u003d 0.0\nevaluation.record_metric(evaluation\u003devaluation_name, key\u003d\u0027utility\u0027, value\u003dutility)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": "evaluation.write_metrics()"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}