{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "! jt -t grade3 -nf opensans"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "\u003ch1\u003e\u003ccenter\u003e1. Load Data\u003c/center\u003e\u003c/h1\u003e\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import warnings\n",
        "import pandas as pd\n",
        "\n",
        "warnings.filterwarnings(action\u003d\u0027ignore\u0027, message\u003d\u0027numpy.dtype size changed\u0027)\n",
        "warnings.filterwarnings(action\u003d\u0027ignore\u0027, message\u003d\u0027compiletime version 3.5 of module\u0027)\n",
        "\n",
        "if not \u0027workbookDir\u0027 in globals():\n",
        "    workbookDir \u003d os.getcwd()\n",
        "os.chdir(os.path.split(workbookDir)[0])\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W1015 14:54:20.296340 4553602496 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Copyright (C) Synthesized Ltd. - All Rights Reserved\n",
            "License key: EE6B-6720-67A2-32F3-3139-2D31-322D-B531\n",
            "Expires at: 2019-12-31 00:00:00\n"
          ]
        },
        {
          "ename": "ImportError",
          "evalue": "cannot import name \u0027UtilityTesting\u0027 from \u0027synthesized.testing\u0027 (/Users/amandlamabona/PycharmProjects/synthesized/synthesized/testing/__init__.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m\u003cipython-input-2-c433c319cce0\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[0;32m----\u003e 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msynthesized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEvaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msynthesized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mevaluation_utils\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0meval_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mevaluation\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mEvaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_path\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;34m\u0027configs/evaluation/dataset_evaluation.json\u0027\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;34m\u0027james\u0027\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# use this line if you want to run this notbook manually\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mevaluation_name\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u0027EVALUATION_NAME\u0027\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\u0027n/a\u0027\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbranch\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u0027EVALUATION_BRANCH\u0027\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\u0027n/a\u0027\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/PycharmProjects/synthesized/synthesized/testing/__init__.py\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mevaluation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEvaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mlinkage_attack\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinkageAttackTesting\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutility\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mUtilityTesting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0m__all__\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\u0027Evaluation\u0027\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\u0027UtilityTesting\u0027\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\u0027LinkageAttackTesting\u0027\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\u0027Column\u0027\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/PycharmProjects/synthesized/synthesized/testing/utility.py\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhighdim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHighDimSynthesizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 32\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msynthesized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mevaluation_utils\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0meval_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mCOLOR_ORIG\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0;34m\u0027#00AB26\u0027\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/PycharmProjects/synthesized/synthesized/testing/evaluation_utils.py\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspearmanr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msynthesized\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHighDimSynthesizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msynthesized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mUtilityTesting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtsa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstattools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0macf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpacf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformula\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmnlogit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name \u0027UtilityTesting\u0027 from \u0027synthesized.testing\u0027 (/Users/amandlamabona/PycharmProjects/synthesized/synthesized/testing/__init__.py)"
          ]
        }
      ],
      "source": "from synthesized.testing.evaluation import Evaluation\nfrom synthesized.testing import evaluation_utils as eval_utils\nfrom synthesized.testing import plotting as syn_plot\n#evaluation \u003d Evaluation(config_path\u003d\u0027configs/evaluation/dataset_evaluation.json\u0027, name\u003d\u0027james\u0027) # use this line if you want to run this notbook manually\nevaluation_name \u003d os.environ.get(\u0027EVALUATION_NAME\u0027, \u0027n/a\u0027)\nbranch \u003d os.environ.get(\u0027EVALUATION_BRANCH\u0027, \u0027n/a\u0027)\nrevision \u003d os.environ.get(\u0027EVALUATION_REVISION\u0027, \u0027n/a\u0027)\nevaluation \u003d Evaluation(branch\u003dbranch, revision\u003drevision, group\u003d\"dataset_evaluation\")\n\nconfig_path \u003d os.environ.get(\u0027EVALUATION_CONFIG_PATH\u0027, \u0027n/a\u0027)\nwith open(config_path, \u0027r\u0027) as f:\n    configs \u003d json.load(f)\n    evaluation.record_config(evaluation\u003devaluation_name, config\u003dconfigs[\"instances\"][evaluation_name])\n        "
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "data \u003d pd.read_csv(evaluation.configs[evaluation_name][\u0027data\u0027])\n",
        "data \u003d data.drop(evaluation.configs[evaluation_name][\u0027ignore_columns\u0027], axis\u003d1)\n",
        "data.dropna(inplace\u003dTrue)\n",
        "data.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\u003ch1\u003e\u003ccenter\u003e2. Train model and generate synthetic data\u003c/center\u003e\u003c/h1\u003e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {},
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "train, test \u003d train_test_split(data, test_size\u003d0.2, random_state\u003d0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {},
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "synthesizers_and_results \u003d [eval_utils.train_and_synthesize(data\u003ddata, evaluation\u003devaluation, \n",
        "                                                            evaluation_name\u003devaluation_name, \n",
        "                                                            test\u003dtest, train\u003dtrain) \n",
        "                            for i in range(evaluation.configs[evaluation_name][\u0027num_passes\u0027])]\n",
        "synthesizer \u003d synthesizers_and_results[0][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## Plot losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "loss_history \u003d synthesizers_and_results[0][2]\n",
        "pd.DataFrame.from_records(loss_history).plot(figsize\u003d(15,7))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## Display aggregated statistics "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": "syn_plot.plot_avg_distances(evaluation\u003devaluation, evaluation_name\u003devaluation_name, \n                            test\u003dtest, results\u003dsynthesizers_and_results)"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## Details for the 1st Run "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "from synthesized.testing import UtilityTesting\n",
        "testing \u003d UtilityTesting(synthesizer, train, test, synthesizers_and_results[0][1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "testing.show_distribution_distances()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "testing.show_distributions(remove_outliers\u003d0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## Display correlations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "testing.show_corr_distances()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "testing.show_corr_matrices()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% Display auto-associations\n"
        }
      },
      "outputs": [],
      "source": [
        "testing.show_auto_associations()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## Demonstrate the utility for training ML models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    utility \u003d testing.utility(target\u003devaluation.configs[evaluation_name][\u0027target\u0027])\n",
        "except:\n",
        "    utility \u003d 0.0\n",
        "evaluation.record_metric(evaluation\u003devaluation_name, key\u003d\u0027utility\u0027, value\u003dutility)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "evaluation.write_metrics()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}