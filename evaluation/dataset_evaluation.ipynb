{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "! jt -t grade3 -nf opensans"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "\u003ch1\u003e\u003ccenter\u003e1. Load Data\u003c/center\u003e\u003c/h1\u003e\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": "import os\nimport json\nimport warnings\nimport pandas as pd\n\nwarnings.filterwarnings(action\u003d\u0027ignore\u0027, message\u003d\u0027numpy.dtype size changed\u0027)\nwarnings.filterwarnings(action\u003d\u0027ignore\u0027, message\u003d\u0027compiletime version 3.5 of module\u0027)\n\nif not \u0027workbookDir\u0027 in globals():\n    workbookDir \u003d os.getcwd()\nos.chdir(os.path.split(workbookDir)[0])\n\n%load_ext autoreload\n%autoreload 2\n%matplotlib inline"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": "from synthesized.testing.evaluation import Evaluation\nfrom synthesized.testing import evaluation_utils as eval_utils\n#evaluation \u003d Evaluation(config_path\u003d\u0027configs/evaluation/dataset_evaluation.json\u0027, name\u003d\u0027james\u0027) # use this line if you want to run this notbook manually\nevaluation_name \u003d os.environ.get(\u0027EVALUATION_NAME\u0027, \u0027n/a\u0027)\nbranch \u003d os.environ.get(\u0027EVALUATION_BRANCH\u0027, \u0027n/a\u0027)\nrevision \u003d os.environ.get(\u0027EVALUATION_REVISION\u0027, \u0027n/a\u0027)\nevaluation \u003d Evaluation(branch\u003dbranch, revision\u003drevision, group\u003d\"dataset_evaluation\")\n\nconfig_path \u003d os.environ.get(\u0027EVALUATION_CONFIG_PATH\u0027, \u0027n/a\u0027)\nwith open(config_path, \u0027r\u0027) as f:\n    configs \u003d json.load(f)\n    evaluation.record_config(evaluation\u003devaluation_name, config\u003dconfigs[\"instances\"][evaluation_name])\n        "
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": "data \u003d pd.read_csv(evaluation.configs[evaluation_name][\u0027data\u0027])\ndata \u003d data.drop(evaluation.configs[evaluation_name][\u0027ignore_columns\u0027], axis\u003d1)\ndata.dropna(inplace\u003dTrue)\ndata.head(5)"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\u003ch1\u003e\u003ccenter\u003e2. Train model and generate synthetic data\u003c/center\u003e\u003c/h1\u003e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {},
        "scrolled": true
      },
      "outputs": [],
      "source": "from sklearn.model_selection import train_test_split"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": "train, test \u003d train_test_split(data, test_size\u003d0.2, random_state\u003d0)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {},
        "scrolled": true
      },
      "outputs": [],
      "source": "%%capture\nsynthesizers_and_results \u003d [eval_utils.train_and_synthesize(data\u003ddata, evaluation\u003devaluation, \n                                                            evaluation_name\u003devaluation_name, \n                                                            test\u003dtest, train\u003dtrain) \n                            for i in range(evaluation.configs[evaluation_name][\u0027num_passes\u0027])]\nsynthesizer \u003d synthesizers_and_results[0][0]"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## Plot losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "loss_history \u003d synthesizers_and_results[0][2]\n",
        "pd.DataFrame.from_records(loss_history).plot(figsize\u003d(15,7))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": "## Display aggregated statistics "
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": "eval_utils.plot_avg_distances(evaluation\u003devaluation, evaluation_name\u003devaluation_name, \n                              test\u003dtest, results\u003dsynthesizers_and_results)"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## Details for the 1st Run "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "from synthesized.testing import UtilityTesting\n",
        "testing \u003d UtilityTesting(synthesizer, train, test, synthesizers_and_results[0][1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "testing.show_distribution_distances()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "testing.show_distributions(remove_outliers\u003d0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## Display correlations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "testing.show_corr_distances()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": "testing.show_corr_matrices()"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "testing.show_auto_associations()",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% Display auto-associations\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": "## Demonstrate the utility for training ML models"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": "try:\n    utility \u003d testing.utility(target\u003devaluation.configs[evaluation_name][\u0027target\u0027])\nexcept:\n    utility \u003d 0.0\nevaluation.record_metric(evaluation\u003devaluation_name, key\u003d\u0027utility\u0027, value\u003dutility)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "evaluation.write_metrics()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}